{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Wrangling\n",
    "# Extracting JSON Files\n",
    "\n",
    "The following series of notebooks are seperated for convinence. Data Wrangling was seperated into multiple parts for convience. \n",
    "\n",
    "## Datasets\n",
    "\n",
    "There are two datasets we are working with for this project. The first dataset is the United Healthcare Insurance Claims dataset, which was just released in December 2022. This dataset is divided into multiple files and will be the main source of files we work with in this notbook. Unfortunately for us, these files are large json files that are not in a format that we can use at this time. In this notebook I hope to obtain the negotiated rates that we need for all procedures availible within this dataset. \n",
    "\n",
    "The second dataset comes from the Centers for Medicare & Medicaid Services (CMS). This includes important metrics and indentifiers for specific hospitals that the US government pays for services. We will explore this dataset furthure in other notebooks.\n",
    "\n",
    "The focus of this notebook is to organize the United Healtcare Insurance Claims dataset into CSV files that we can explore futhure. \n",
    "\n",
    "### United Healthcare Insurance Claims Dataset\n",
    "\n",
    "As stated before, these are large json files. This data contains information about all claims made by United Health Incurance clients. They contain information about the procedures done (eg. drug purchases to performing surgery) and which health care provider performed the service. I will explain in more detail how these files are organized.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Here I import the packages that I need. All the packages can be downloaded through conda or pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #we shall be requesting packages to download from UHC insurance website\n",
    "import shutil\n",
    "import ijson.backends.python as ijson # a json file parser\n",
    "import json\n",
    "import gzip \n",
    "from tqdm.auto import tqdm\n",
    "from csv import writer\n",
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Organization\n",
    "\n",
    "To get started, its important to note how the data is available to us. As stated prior, the data is availiable online at: \n",
    "\n",
    "[United Healthcare Insurance Dataset](https://transparency-in-coverage.uhc.com/?_gl=1*5it7ok*_ga*NjMzOTkzMDA0LjE2NzI3OTc4MjA.*_ga_HZQWR2GYM4*MTY3Mjc5NzgyMC4xLjAuMTY3Mjc5NzgyMC4wLjAuMA)\n",
    "\n",
    "You can get some information about the data is organized through this git repository:\n",
    "\n",
    "[Git Respository](https://github.com/CMSgov/price-transparency-guide)\n",
    "\n",
    "\n",
    "For summation, some basic facts about the organization of the UHC dataset:\n",
    "\n",
    "<img src=\"Claimsprocess2noterms.jpeg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Reference: Claimsprocess2noterms.jpeg. Blue Cross NC. (n.d.). Retrieved March 12, 2023, from https://www.bluecrossnc.com/file/claimsprocess2notermsjpeg\n",
    "\n",
    "**I.  In-Network vs Out-of-Network**  \n",
    "UHC has several files for download on their site. These files are organized in-network and out-of-network. Since we are interested is negotiated rates between hospitals and insurance companies, we are only going to be working with datasets that are in-network. UHC would not negotiate with providers for out-of-network rates and these rates are most likely standardized by the provider (ex. any customer that buys a drug without insurance from a local drugstore would pay a price set by the drugstore, not a negotiated rate). These json files are stored on an Azure server and need to be downloaded individually and parsed. Since there are hundreds of these files, I created an excel spreadsheet called <font color=green>_json_files_hyperlinks.xlsx_</font> with hyperlinks to each of the json files we are interested in.  \n",
    "\n",
    "**II.  Insured Groups**  \n",
    "The json files are seperated by insured group. For example, let us say you work for company X, and company X insures all of their employees with United Healthcare insurance. There would be a specific json file for company X in this Azure database. However, some insured groups are exceptionally large (think of a large company like amazon or google), and therefore are split up into multiple json files. \n",
    "\n",
    "**III. Negotiated Rates**  \n",
    "Negotiated rates are often times repeated in these files because different insured groups are going to purchase the same procedure from the same provider at a negotiated rate; however, different insurance plans may exist. For example, Mr. Smith and Mrs. Nancy both work for company X and both live in the same area. They may both purchase insulin from the same drugstore through their United Healthcare Insurance plan. If they have the same plan through their employer, the price negoitated would most likely be the same; however, say Mr. Smith purchased a plan with some level of higher coverage through United Healthcare, his negotiated price may be reduced.\n",
    "\n",
    "**IV. JSON Files**  \n",
    "Each json file is divided into two groups that refer to each other: \n",
    "1. *Provider groups*  \n",
    "These refer to hospitals, pharmacies, or private practice physicians:\n",
    "    - Reference Number: A number used to refer to a group of providers (next subullet point). This number is specific for this json file and does not transfer to another json file.\n",
    "    - NPI provider Groups: This is an array of National Provider Identifiers(NPIs). NPIs are unique identification numbers for covered health care providers. Each NPI refers to a provider and can be tracked through the CMS database. Sometimes one provider may have several NPIs associated with them. For example, a doctor that practices as 2 hospitals may have 2 seperate identifiers.\n",
    "2. *Billing Information*  \n",
    "    - Billing Type and Billing Value: In the US there are several standardized ways to bill a service/procedure/drug. The billing type refers to one of these standarized methods and the value can be interpreted to a service/procedure/drug. Examples of different standarized billing types include CPT, HCPCS and MS-DRG. An example is code type CPT(Current Procedural Terminology created by the American Medical Association) and code value 36415 refers to a Routine Venipucture that can be searched on the American Medical Association codebook. These different billing methods exist because sometimes an insurance company may pay for a drug individually, in which case they would use the CPT code for that drug, or they may pay a bulk sum for a surgery that may include several items (drugs, labor, hospital stay, etc) were they would use MS-DRG or HCPCS code which refers more to procedures. In addition providers may be more comfortable with one billing method versus another. This is complex, but it is important to know there are different billing methods and this refers to the billing type.\n",
    "    - Reference Numbers: Provider groups that provided this service to this insured group\n",
    "    - Negotiated Rates: The negotiated rate with the provider group.\n",
    "\n",
    "_Code Type and Code Number are a majority of these files and are essentially transaction history for United Healthcare for each company they cover_\n",
    "  \n",
    "\n",
    "I will break this down futhure as we move along this notebook. For now I am currently using a dotenv to set up my directory where I am accessing files and storying files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to important files and places where you want to store files\n",
    "load_dotenv()\n",
    "\n",
    "# This is the file with the hyperlinks of json files we need to parse\n",
    "hyperlink_path = 'json_files_hyperlinks.xlsx'\n",
    "hyperlinks = pd.read_excel(hyperlink_path)['Hyperlinks'].tolist()\n",
    "\n",
    "# I intent to store the json files to be downloaded alongside creating folders for the parsed data: These are large files so this will be done on an external hard drive.\n",
    "parent_dir= os.getenv('dir')\n",
    "dir_json = os.path.join(parent_dir, 'JSON')\n",
    "dir_data = os.path.join(parent_dir,'data_update')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Parsing JSON files\n",
    "\n",
    "The follow sections of code are used to parse the dataset available. I have some predefined fuctions, all of which I do not explicitly use in this notebook, but can be useful for your own work with this dataset depending on how many files you want to download and parse.\n",
    "\n",
    "### Downloading JSON files\n",
    "  \n",
    "\n",
    "Here are a list of functions for downloading files from the [United Healthcare Insurance Dataset](https://transparency-in-coverage.uhc.com/?_gl=1*5it7ok*_ga*NjMzOTkzMDA0LjE2NzI3OTc4MjA.*_ga_HZQWR2GYM4*MTY3Mjc5NzgyMC4xLjAuMTY3Mjc5NzgyMC4wLjAuMA). Again, we will be only working with in-network files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads one file given a url and path where the file should be stored. It returns the filename and location where it has been downloaded.\n",
    "def download_file(url, path):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    download_path = os.path.join(path,local_filename)\n",
    "    if os.path.exists(download_path):\n",
    "        print(download_path + '\\nFile already Exists!')\n",
    "        return (local_filename, download_path)\n",
    "    else:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            total_length = int(r.headers.get('content-length'))\n",
    "            with tqdm.wrapattr(r.raw, \"read\", total=total_length, desc=\"\")as raw:\n",
    "                with open(download_path, 'wb') as output:\n",
    "                    shutil.copyfileobj(raw, output)\n",
    "        print(local_filename +'\\nDownload Complete')\n",
    "        return (local_filename, download_path)\n",
    "\n",
    "# This downloads more than one file given a url and path where the file should be stored. It returns the filename and location where it has been downloaded.\n",
    "def download_multiple_files(urls, path):\n",
    "    local_filenames = []\n",
    "    download_paths = []\n",
    "    for url in urls:\n",
    "        local_filename = url.split('/')[-1]\n",
    "        download_path = os.path.join(path,local_filename)\n",
    "        if os.path.exists(download_path):\n",
    "            print(download_path + '\\nFile already Exists!')\n",
    "            local_filenames.append(local_filename) \n",
    "            download_paths.append(download_path)\n",
    "        else:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                total_length = int(r.headers.get('content-length'))\n",
    "                with tqdm.wrapattr(r.raw, \"read\", total=total_length, desc=\"\")as raw:\n",
    "                    with open(download_path, 'wb') as output:\n",
    "                        shutil.copyfileobj(raw, output)\n",
    "            print(local_filename +'\\nDownload Complete!')\n",
    "            local_filenames.append(local_filename) \n",
    "            download_paths.append(download_path)\n",
    "    return (local_filenames, download_paths)\n",
    "\n",
    "# This checks the file size before the file is downloaded. This is useful if you want to avoid large files.\n",
    "def check_file_size(url):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        return int(r.headers.get('content-length'))\n",
    "\n",
    "\n",
    "# This deletes a file selected from a path. If you want to work with one file at a time, this is handy.\n",
    "def delete_file(path):\n",
    "    os.remove(path)\n",
    "    print(path + '\\nFile Deleted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Path\n",
    "This function simply makes a folder name that matchs the hyperlink file to store the files we create from one downloaded JSON file after the data is parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_paths_folders(filename, json_file):\n",
    "    folder_name = filename[0:-8]\n",
    "    path= os.path.join(dir_data, folder_name)\n",
    "    if os.path.exists(path) is False:\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        print(path + '\\nFolder Already Exists')\n",
    "    return path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVs for completed/skipped files\n",
    "\n",
    "\n",
    "The following code is to keep track of what files we have parsed. Since this is a large data set, it might be worth your time to split up the work as the data is parsed. Some of the files might be too large, in which case skipping the larger files may be worth it. I found it useful to create seperate CSVs for the files I skipped and the files I completed incase I wanted to go back and wanted to parse some larger files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the file was too large, it will be written to a certain CSV\n",
    "def write_large_file(filename, index, hyperlink):\n",
    "    with open(filename, 'a') as f:\n",
    "        writer_object = writer(f)\n",
    "        writer_object.writerow([index,hyperlink])\n",
    "        print('File Too Large, written to Large File CSV')\n",
    "        f.close()\n",
    "\n",
    "#if the file is parsed, it will be written to a different CSV compared to above\n",
    "def write_completed_file(filename, index, hyperlink):\n",
    "    with open(filename, 'a') as f:\n",
    "        writer_object = writer(f)\n",
    "        writer_object.writerow([index,hyperlink])\n",
    "        print('File has been completed')\n",
    "        f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing information\n",
    "\n",
    "The following code are fuctions related to storing the parsed information. These files take in lists of values and store them as CSV files. I decided to create two seperate CSVs to seperate the provider group information from the billing information since they are seperate tables. However, since reference group link the two tables together, I usually store these files in the same location with similar names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_provider_csv(filename, reference, tin, npi_provider_groups):\n",
    "    with open(filename, 'a') as f:\n",
    "        writer_object = writer(f)\n",
    "        for i,r in enumerate(reference):\n",
    "            writer_object.writerow([r,tin[i],npi_provider_groups[i]])\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def write_rates_csv(filename, billing_type, billing_code, provider_reference, rate):\n",
    "    with open(filename, 'a') as f:\n",
    "        writer_object = writer(f)\n",
    "        for i, ref in enumerate(provider_reference):\n",
    "            writer_object.writerow([billing_type[i],billing_code[i],ref,rate[i]])\n",
    "        f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing JSON File\n",
    "\n",
    "The following code is responsible for parsing the JSON file. It goes through the JSON code line by line and matchs the values we are interested in and stores them. The parse_file function contains two main sections, provider groups and billing information. As stated above, these files are divided into these two sections and we are interested in both. This function utilizes some of the predefined functions from above.\n",
    "\n",
    "_There are additional values in these files not explained in the summary section. For example 'tin' refers to a buisness's tax identification number and is difficult to look up. Storing this value, since it is an unique identifier for NPI Provider Group, may come in handy. Other values were ignored._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename, json_file):\n",
    "    # creates a folder to store the two CSV files we intend to create\n",
    "    path = make_paths_folders(filename, json_file)\n",
    "\n",
    "    # intiates the 2 CSV files we intend to write\n",
    "    providers_csv = os.path.join(path, filename[0:-8]) + '_providers.csv'\n",
    "    rates_csv = os.path.join(path, filename[0:-8]) + '_rates.csv'\n",
    "    write_provider_csv(providers_csv, ['provider_reference'], ['tin'], ['npi_provider_groups'])\n",
    "    write_rates_csv(rates_csv, ['billing_type'], ['billing_code'], ['provider_reference'], ['negotiated_rates'])\n",
    "\n",
    "    # intiating lists to store Provider Group values\n",
    "    npi_provider_groups = []\n",
    "    tin = []\n",
    "    reference = []\n",
    "    \n",
    "    # intiating lists to store Billing Information values\n",
    "    billing_type = []\n",
    "    billing_code = []\n",
    "    ref_group = []\n",
    "    rates = []\n",
    "    \n",
    "    # opens the files to parse and parses the values\n",
    "    with gzip.open(json_file, mode=\"rt\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "\n",
    "        for prefix, event, value in tqdm(parser):\n",
    "            # if you have limited RAM, this insures that your lists do not get too long before you write them on your hard drive\n",
    "            if len(npi_provider_groups) >= 1000:\n",
    "                write_provider_csv(providers_csv, reference, tin, npi_provider_groups)\n",
    "                npi_provider_groups = []\n",
    "                tin = []\n",
    "                reference = []\n",
    "            if len(rates) >= 10000:\n",
    "                write_rates_csv(rates_csv, billing_type, billing_code, ref_group, rates)\n",
    "                billing_type = []\n",
    "                billing_code = []\n",
    "                ref_group = []\n",
    "                rates = []\n",
    "\n",
    "            # This is for parsing realavant provider group information\n",
    "            if prefix =='provider_references.item.provider_groups.item.npi' and event =='start_array'and value==None:\n",
    "                temp_npi = []\n",
    "            elif prefix =='provider_references.item.provider_groups.item.npi.item' and event =='number':\n",
    "                temp_npi.append(value)\n",
    "            elif prefix =='provider_references.item.provider_groups.item.tin.value' and event =='string':\n",
    "                temp_tin =value\n",
    "            elif prefix =='provider_references.item.provider_group_id' and event =='number':\n",
    "                npi_provider_groups.append(temp_npi)\n",
    "                tin.append(temp_tin)\n",
    "                reference.append(value)\n",
    "            # This informs us we have reached the end of the provider group information\n",
    "            elif prefix =='provider_references' and event =='end_array':\n",
    "                write_provider_csv(providers_csv, reference, tin, npi_provider_groups)\n",
    "                npi_provider_groups = []\n",
    "                tin = []\n",
    "                reference = []\n",
    "            # This is for storing the billing information values\n",
    "            elif prefix =='in_network.item.billing_code_type' and event =='string':\n",
    "                temp_type = value\n",
    "            elif prefix =='in_network.item.billing_code' and event =='string':\n",
    "                temp_code = value\n",
    "            elif prefix =='in_network.item.negotiated_rates.item.provider_references.item' and event =='number':\n",
    "                temp_ref = value\n",
    "            elif prefix =='in_network.item.negotiated_rates.item.negotiated_prices.item.negotiated_rate' and event =='number':\n",
    "                billing_type.append(temp_type)\n",
    "                billing_code.append(temp_code)\n",
    "                ref_group.append(temp_ref)\n",
    "                rates.append(value)\n",
    "            # This informs us we have reached the end of the billing information values\n",
    "            elif prefix =='in_network' and event =='end_array':\n",
    "                write_rates_csv(rates_csv, billing_type, billing_code, ref_group, rates)\n",
    "                billing_type = []\n",
    "                billing_code = []\n",
    "                ref_group = []\n",
    "                rates = []\n",
    "        f.close()\n",
    "    print(json_file + '\\nParse Complete')    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination functions\n",
    "\n",
    "These files combine several of the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parse(url, path):\n",
    "    (filename, json_file) = download_file(url, path)\n",
    "    parse_file(filename, json_file)\n",
    "\n",
    "\n",
    "def download_parse_delete(url, path):\n",
    "    (filename, json_file) = download_file(url, path)\n",
    "    parse_file(filename, json_file)\n",
    "    delete_file(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Here you can store the the file you want to access for later since the following steps may take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://uhc-tic-mrf.azureedge.net/public-mrf/2023-01-01/2023-01-01_UnitedHealthcare-Insurance-Company_Insurer_D0015336_UHC-Dental_in-network-rates.json.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperlinks[2949]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Parse\n",
    "\n",
    "Feel free to change the following variables:\n",
    "- <font color=blue>start</font>: variable to determine where you want to start your current run.\n",
    "- <font color=blue>number_to_attempt_to_parse</font>: variable to determine how many files you attempt to parse for this current run. \n",
    "- <font color=blue>file_size_max</font>: determines which file sizes you are comfortable parsing. This can be affected by your CPU speed, RAM, and internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_attempt_to_parse = 1000\n",
    "start = 2949\n",
    "file_size_max = 100_000_000\n",
    "\n",
    "large_files = 'json_large_hyperlinks_update.csv'\n",
    "completed = 'json_completed_hyperlinks_update.csv'\n",
    "\n",
    "for i in range(start,start + number_to_attempt_to_parse):\n",
    "    if check_file_size(hyperlinks[i]) < file_size_max: \n",
    "        print('Hyperlink File: ' + str(i) + ' Started!')\n",
    "        download_parse_delete(hyperlinks[i], dir_json)\n",
    "        write_completed_file(completed, i, hyperlinks[i])\n",
    "    else:\n",
    "        write_large_file(large_files,i,hyperlinks[i])\n",
    "        print('File number: ' + str(i))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "We now have CSV files of Provider Groups and Billing Information. Our next objective is to combine these files in a way that makes sense and determine which NPI provider groups are associated with a certain hospital.\n",
    "\n",
    "Some compromises we made during this notebook include avoiding large files and not being able to parse all the data. On way to make this notebook better would be to parse random files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:28:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
