{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"CFG = {'TPU': 0,\n       'block_size': 12096, \n       'block_stride': 12096//16,\n       'patch_size': 14, \n       \n       'fog_model_dim': 320,\n       'fog_model_num_heads': 6,\n       'fog_model_num_encoder_layers': 5,\n       'fog_model_num_lstm_layers': 2,\n       'fog_model_first_dropout': 0.1,\n       'fog_model_encoder_dropout': 0.1,\n       'fog_model_mha_dropout': 0.0,\n      }\n\nassert CFG['block_size'] % CFG['patch_size'] == 0\nassert CFG['block_size'] % CFG['block_stride'] == 0\n\n'''\nMean-std normalization function. \nExample input: shape (5000), dtype np.float32\nExample output: shape (5000), dtype np.float32\n\nUsed to normalize AccV, AccML, AccAP values.\n\n'''\n\ndef sample_normalize(sample):\n    mean = tf.math.reduce_mean(sample)\n    std = tf.math.reduce_std(sample)\n    sample = tf.math.divide_no_nan(sample-mean, std)\n    \n    return sample.numpy()\n\n'''\nFunction for splitting a series into blocks. Blocks can overlap. \nHow the function works:\nSuppose we have a series with AccV, AccML, AccAP columns and len of 50000, that is (50000, 3). \nFirst, the series is padded so that the final length is divisible by CFG['block_size'] = 15552. Now the series shape is (62208, 3).\nThen we get blocks: first block is series[0:15552, :], second block is series[972:16524, :], ... , last block is series[46656:62208, :].\n\n'''\n\ndef get_blocks(series, columns):\n    series = series.copy()\n    series = series[columns]\n    series = series.values\n    series = series.astype(np.float32)\n    \n    block_count = math.ceil(len(series) / CFG['block_size'])\n    \n    series = np.pad(series, pad_width=[[0, block_count*CFG['block_size']-len(series)], [0, 0]])\n    \n    block_begins = list(range(0, len(series), CFG['block_stride']))\n    block_begins = [x for x in block_begins if x+CFG['block_size'] <= len(series)]\n    \n    blocks = []\n    for begin in block_begins:\n        values = series[begin:begin+CFG['block_size']]\n        blocks.append({'begin': begin,\n                       'end': begin+CFG['block_size'],\n                       'values': values})\n    \n    return blocks\n\n'''\nTrain and inference batch size\n\n'''\n\nGPU_BATCH_SIZE = 16\nTPU_BATCH_SIZE = GPU_BATCH_SIZE*8","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T12:41:32.870796Z","iopub.execute_input":"2023-06-25T12:41:32.871158Z","iopub.status.idle":"2023-06-25T12:41:32.884761Z","shell.execute_reply.started":"2023-06-25T12:41:32.871130Z","shell.execute_reply":"2023-06-25T12:41:32.883768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport warnings\n\nif CFG['TPU']:\n    !pip install -q /lib/wheels/tensorflow-2.9.1-cp38-cp38-linux_x86_64.whl\n    !pip install -qU scikit-learn\n    \nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport scipy\n\nfrom tqdm import tqdm\nfrom itertools import cycle\nfrom joblib import Parallel, delayed\nfrom sklearn.metrics import average_precision_score\n\nif CFG['TPU']:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='local') \n    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_colwidth', None)\n\ndef folder(path): \n    if not os.path.exists(path): os.makedirs(path)\n        \ndef plot(e, size=(20, 4)):\n    plt.figure(figsize=size)\n    plt.plot(e)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T12:41:32.886887Z","iopub.execute_input":"2023-06-25T12:41:32.887295Z","iopub.status.idle":"2023-06-25T12:41:32.906324Z","shell.execute_reply.started":"2023-06-25T12:41:32.887259Z","shell.execute_reply":"2023-06-25T12:41:32.905153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThe transformer encoder layer\nFor more details, see https://arxiv.org/pdf/1706.03762.pdf [Attention Is All You Need]\n\n'''\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        \n        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=CFG['fog_model_num_heads'], key_dim=CFG['fog_model_dim'], dropout=CFG['fog_model_mha_dropout'])\n        \n        self.add = tf.keras.layers.Add()\n        \n        self.layernorm = tf.keras.layers.LayerNormalization()\n        \n        self.seq = tf.keras.Sequential([tf.keras.layers.Dense(CFG['fog_model_dim'], activation='relu'), \n                                        tf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']), \n                                        tf.keras.layers.Dense(CFG['fog_model_dim']), \n                                        tf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']),\n                                       ])\n        \n    def call(self, x):\n        attn_output = self.mha(query=x, key=x, value=x)\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n        x = self.add([x, self.seq(x)])\n        x = self.layernorm(x)\n        \n        return x\n    \n'''\nFOGEncoder is a combination of transformer encoder (D=320, H=6, L=5) and two BidirectionalLSTM layers\n\n'''\n\nclass FOGEncoder(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        \n        self.first_linear = tf.keras.layers.Dense(CFG['fog_model_dim'])\n        \n        self.add = tf.keras.layers.Add()\n        \n        self.first_dropout = tf.keras.layers.Dropout(CFG['fog_model_first_dropout'])\n        \n        self.enc_layers = [EncoderLayer() for _ in range(CFG['fog_model_num_encoder_layers'])]\n        \n        self.lstm_layers = [tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(CFG['fog_model_dim'], return_sequences=True)) for _ in range(CFG['fog_model_num_lstm_layers'])]\n        \n        self.sequence_len = CFG['block_size'] // CFG['patch_size']\n        self.pos_encoding = tf.Variable(initial_value=tf.random.normal(shape=(1, self.sequence_len, CFG['fog_model_dim']), stddev=0.02), trainable=True)\n        \n    def call(self, x, training=None): # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), Example shape (4, 864, 42)\n        x = x / 50.0 # Normalization attempt in the segment [-1, 1]\n        x = self.first_linear(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']), Example shape (4, 864, 320)\n          \n        if training: # augmentation by randomly roll of the position encoding tensor\n            random_pos_encoding = tf.roll(tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1]), \n                                          shift=tf.random.uniform(shape=(GPU_BATCH_SIZE,), minval=-self.sequence_len, maxval=0, dtype=tf.int32),\n                                          axis=GPU_BATCH_SIZE * [1],\n                                          )\n            x = self.add([x, random_pos_encoding])\n        \n        else: # without augmentation \n            x = self.add([x, tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1])])\n            \n        x = self.first_dropout(x)\n        \n        for i in range(CFG['fog_model_num_encoder_layers']): x = self.enc_layers[i](x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']), Example shape (4, 864, 320)\n        for i in range(CFG['fog_model_num_lstm_layers']): x = self.lstm_layers[i](x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']*2), Example shape (4, 864, 640)\n            \n        return x\n    \nclass FOGModel(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        \n        self.encoder = FOGEncoder()\n        self.last_linear = tf.keras.layers.Dense(4) \n        \n    def call(self, x): # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), Example shape (4, 864, 42)\n        x = self.encoder(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']*2), Example shape (4, 864, 640)\n        x = self.last_linear(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 3), Example shape (4, 864, 4)\n        x = tf.nn.sigmoid(x) # Sigmoid activation\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-25T12:41:32.908856Z","iopub.execute_input":"2023-06-25T12:41:32.909490Z","iopub.status.idle":"2023-06-25T12:41:32.932330Z","shell.execute_reply.started":"2023-06-25T12:41:32.909435Z","shell.execute_reply":"2023-06-25T12:41:32.931124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreate defog and notype train blocks with \nAccV, AccML, AccAP, StartHesitation, Turn, Walking, Event, StartHesitation_mask, Turn_mask, Walking_mask, Event_mask columns \n\n'''\n\nsave_path = '/kaggle/working/train/defog'; folder(save_path); \ndefog_metadata = pd.read_csv('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv').set_index('Id')\n\ndef get_blocks_descriptions(Id):\n    blocks_descriptions = []\n    \n    try: \n        series = pd.read_csv(f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{Id}.csv')\n        series['Event'] = series[['StartHesitation', 'Turn', 'Walking']].aggregate('max', axis=1)\n        \n        series['StartHesitation_mask'] = (series['Task'] & series['Valid']).astype('int')\n        series['Turn_mask'] = (series['Task'] & series['Valid']).astype('int')\n        series['Walking_mask'] = (series['Task'] & series['Valid']).astype('int')\n        series['Event_mask'] = (series['Task'] & series['Valid']).astype('int')\n        \n        source = 'defog'\n        \n    except FileNotFoundError: \n        series = pd.read_csv(f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/notype/{Id}.csv')\n        series['StartHesitation'] = 0\n        series['Turn'] = 0\n        series['Walking'] = 0\n        \n        series['StartHesitation_mask'] = 0\n        series['Turn_mask'] = 0\n        series['Walking_mask'] = 0\n        series['Event_mask'] = (series['Task'] & series['Valid']).astype('int')\n        \n        source = 'notype'\n    \n    series['AccV'] = sample_normalize(series['AccV'].values)\n    series['AccML'] = sample_normalize(series['AccML'].values)\n    series['AccAP'] = sample_normalize(series['AccAP'].values)\n\n    blocks = get_blocks(series, ['AccV', 'AccML', 'AccAP', \n                                 'StartHesitation', 'Turn', 'Walking', 'Event', \n                                 'StartHesitation_mask', 'Turn_mask', 'Walking_mask', 'Event_mask'])\n\n    for block_count, block in enumerate(blocks):\n        fname, values = f'{Id}_{block_count}.npy', block['values']\n        block_description = {}\n        block_description['Id'] = Id\n        block_description['Count'] = block_count\n        block_description['File'] = fname\n        block_description['Path'] = f'{save_path}/{fname}'\n        block_description['Source'] = source\n        \n        block_description['StartHesitation_size'] = np.sum(values[:, 3])\n        block_description['Turn_size'] = np.sum(values[:, 4])\n        block_description['Walking_size'] = np.sum(values[:, 5])\n        block_description['Event_size'] = np.sum(values[:, 6])\n        \n        block_description['StartHesitation_mask_size'] = np.sum(values[:, 7])\n        block_description['Turn_mask_size'] = np.sum(values[:, 8])\n        block_description['Walking_mask_size'] = np.sum(values[:, 9])\n        block_description['Event_mask_size'] = np.sum(values[:, 10])\n        \n        blocks_descriptions.append(block_description)\n        \n        # Load\n        series = values # [AccV, AccML, AccAP, StartHesitation, Turn, Walking, Event, StartHesitation_mask, Turn_mask, Walking_mask, Event_mask]\n        \n        # Create patches\n        series = tf.reshape(series, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], series.shape[1]))\n\n        # Create input\n        series_input = series[:, :, :3]\n        series_input = tf.reshape(series_input, shape=(CFG['block_size'] // CFG['patch_size'], -1))\n\n        # Create target\n        series_target = series[:, :, 3:]\n        series_target = tf.transpose(series_target, perm=[0, 2, 1])\n        series_target = tf.reduce_max(series_target, axis=-1)\n\n        RAM[(Id, block_count)] = (series_input, series_target)\n    \n    blocks_descriptions = pd.DataFrame(blocks_descriptions)\n    \n    return blocks_descriptions\n\nRAM = {}\nblocks_descriptions = [get_blocks_descriptions(defog_metadata.index[i]) for i in tqdm(range(len(defog_metadata.index)), desc='Preparation')]\nblocks_descriptions = pd.concat(blocks_descriptions)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T12:41:32.933969Z","iopub.execute_input":"2023-06-25T12:41:32.934559Z","iopub.status.idle":"2023-06-25T12:43:26.081431Z","shell.execute_reply.started":"2023-06-25T12:41:32.934514Z","shell.execute_reply":"2023-06-25T12:43:26.080313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nSelecting validation subjects\n\n'''\n\nval_subjects = ['00f674', '8d43d9', '107712', '7b2e84', '575c60', '7f8949', '2874c5', '72e2c7']\n\ntrain_ids = defog_metadata[defog_metadata['Subject'].apply(lambda x: x not in val_subjects)].index.tolist()\nval_ids = defog_metadata[defog_metadata['Subject'].apply(lambda x: x in val_subjects)].index.tolist()\n\ntrain_blocks_descriptions = blocks_descriptions[blocks_descriptions['Id'].apply(lambda x: x in train_ids)]\n\nprint(f'\\n[Train ids] {len(train_ids)} [Val ids] {len(val_ids)} ({100*len(val_ids)/(len(train_ids)+len(val_ids)):.1f})')\nprint(f'[Train blocks] {len(train_blocks_descriptions )}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-25T12:43:26.084180Z","iopub.execute_input":"2023-06-25T12:43:26.084618Z","iopub.status.idle":"2023-06-25T12:43:26.170499Z","shell.execute_reply.started":"2023-06-25T12:43:26.084584Z","shell.execute_reply":"2023-06-25T12:43:26.169530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreate a random train dataset from train_blocks_descriptions DataFrame\n\n'''\n\ndef read(row):\n    def read_from_ram(Id, Count):  \n        series_inputs, series_targets = RAM[(Id.numpy().decode('utf-8'), Count.numpy())]\n        return series_inputs, series_targets\n\n    [series_input, series_target] = tf.py_function(read_from_ram, [row['Id'], row['Count']], [tf.float32, tf.float32])\n    series_input.set_shape(shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))\n    series_target.set_shape(shape=(CFG['block_size'] // CFG['patch_size'], 8))\n    \n    return series_input, series_target\n\nids_groups, num_subgroups = [id_group for Id, id_group in train_blocks_descriptions.groupby('Id')], CFG['block_size'] // CFG['block_stride']\nrandom.shuffle(ids_groups)\n\ndataset = []\nfor i in range(num_subgroups):\n    for group in ids_groups:\n        subgroup = group.iloc[i::num_subgroups]\n        dataset.append(subgroup)\n\ndataset = pd.concat(dataset)\ndataset = dataset[dataset['Event_mask_size'] != 0]\ndataset = dataset.reset_index(drop=True); display(dataset);\ndataset = tf.data.Dataset.from_tensor_slices(dict(dataset))\ndataset = dataset.map(read).repeat().batch(TPU_BATCH_SIZE if CFG['TPU'] else GPU_BATCH_SIZE, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T12:43:26.171848Z","iopub.execute_input":"2023-06-25T12:43:26.172189Z","iopub.status.idle":"2023-06-25T12:43:26.715034Z","shell.execute_reply.started":"2023-06-25T12:43:26.172158Z","shell.execute_reply":"2023-06-25T12:43:26.714092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nloss_function args exp\n\nreal is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 8) where the last axis means:\n0 - StartHesitation \n1 - Turn\n2 - Walking\n3 - Event\n4 - StartHesitation mask\n5 - Turn mask\n6 - Walking mask\n7 - Event mask\n\noutput is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 4) where the last axis means:\n0 - StartHesitation predicted\n1 - Turn predicted\n2 - Walking predicted\n3 - Event predicted\n\n'''\n\nce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n\ndef loss_function(real, output, name='loss_function'):\n    loss = ce(tf.expand_dims(real[:, :, 0:4], axis=-1), tf.expand_dims(output, axis=-1)) # Example shape (32, 864, 4)\n    \n    mask = real[:, :, 4:8] # Example shape (32, 864, 4)\n    mask = tf.cast(mask, dtype=loss.dtype)\n    loss *= mask # Example shape (32, 864, 4)\n    \n    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n'''\nSimple learning rate schedule with warm up steps\n\n'''\n        \nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, initial_lr, warmup_steps=1):\n        super(CustomSchedule, self).__init__()\n\n        self.initial_lr = tf.cast(initial_lr, tf.float32)\n        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        return tf.math.minimum(self.initial_lr, self.initial_lr * (step/self.warmup_steps))\n    \n'''\nPredictionFnCallback is used for:\n1. Loading validation data\n2. FOGModel data preparation\n3. Prediction\n4. Scoring and save\n\n'''\n    \nclass PredictionFnCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, model=None, verbose=0):\n        \n        if not model is None: self.model = model\n        self.verbose = verbose\n         \n        def init(Id, path):\n            try:\n                series = pd.read_csv(path).reset_index(drop=True)\n            except FileNotFoundError:\n                return\n            \n            series['Id'] = Id\n            series['AccV'] = sample_normalize(series['AccV'].values)\n            series['AccML'] = sample_normalize(series['AccML'].values)\n            series['AccAP'] = sample_normalize(series['AccAP'].values)\n            \n            try:\n                series['Event'] = series[['StartHesitation', 'Turn', 'Walking']].aggregate('max', axis=1)\n            except:\n                pass\n            \n            series_blocks=[]\n            for block in get_blocks(series, ['AccV', 'AccML', 'AccAP']): # Example shape (12096, 3)\n                values = tf.reshape(block['values'], shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], 3)) # Example shape (864, 14, 3)\n                values = tf.reshape(values, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3)) # Example shape (864, 42)\n                values = tf.expand_dims(values, axis=0) # Example shape (1, 864, 42)\n                \n                self.blocks.append(values)\n                series_blocks.append((self.blocks_counter, block['begin'], block['end']))\n                self.blocks_counter += 1\n            \n            description = {}\n            description['series'] = series\n            description['series_blocks'] = series_blocks\n            self.descriptions.append(description)\n            \n        self.descriptions = [] # Blocks metadata\n        self.blocks = [] # Validation data blocks\n        self.blocks_counter=0 # Blocks counter\n                \n        defog_ids = val_ids\n        defog_paths = [f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{defog_id}.csv' for defog_id in defog_ids]\n        for defog_id, defog_path in tqdm(zip(defog_ids, defog_paths), total=len(defog_ids), desc='PredictionFnCallback Initialization', disable=1-verbose): \n            init(defog_id, defog_path)\n                \n        defog_ids = val_ids\n        defog_paths = [f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/notype/{defog_id}.csv' for defog_id in defog_ids]\n        for defog_id, defog_path in tqdm(zip(defog_ids, defog_paths), total=len(defog_ids), desc='PredictionFnCallback Initialization', disable=1-verbose): \n            init(defog_id, defog_path)\n            \n        self.blocks = tf.concat(self.blocks, axis=0) # Example shape (self.blocks_counter, 864, 42)\n        \n        '''\n        self.blocks is padded so that the final length is divisible by inference batch size for error-free operation of model.predict function\n        Padded values have no effect on the predictions\n        \n        '''\n        \n        self.blocks = tf.pad(self.blocks, \n                             paddings=[[0, math.ceil(self.blocks_counter / (TPU_BATCH_SIZE if CFG['TPU'] else GPU_BATCH_SIZE))*(TPU_BATCH_SIZE if CFG['TPU'] else GPU_BATCH_SIZE)-self.blocks_counter], \n                                                    [0, 0], \n                                                    [0, 0],\n                                      ]) # Example shape (self.blocks_counter+pad_value, 864, 42)\n        \n        print(f'\\n[PredictionFnCallback Initialization] [Series] {len(self.descriptions)} [Blocks] {self.blocks_counter}\\n')\n    \n    def prediction(self):\n        predictions = model.predict(self.blocks, batch_size=TPU_BATCH_SIZE if CFG['TPU'] else GPU_BATCH_SIZE, verbose=self.verbose) # Example shape (self.blocks_counter+pad_value, 864, 4)\n        predictions = predictions[:, :, :3] # Example shape (self.blocks_counter+pad_value, 864, 3)\n        predictions = tf.expand_dims(predictions, axis=-1) # Example shape (self.blocks_counter+pad_value, 864, 3, 1)\n        predictions = tf.transpose(predictions, perm=[0, 1, 3, 2]) # Example shape (self.blocks_counter+pad_value, 864, 1, 3)\n        predictions = tf.tile(predictions, multiples=[1, 1, CFG['patch_size'], 1]) # Example shape (self.blocks_counter+pad_value, 864, 14, 3)\n        predictions = tf.reshape(predictions, shape=(predictions.shape[0], predictions.shape[1]*predictions.shape[2], 3)) # Example shape (self.blocks_counter+pad_value, 12096, 3)\n        predictions = predictions.numpy()\n        \n        '''\n        The following function aggregates predictions blocks and creates dataframes with StartHesitation_prediction, Turn_prediction, Walking_prediction columns.\n        \n        '''\n        \n        def create_target(description):\n            series, series_blocks = description['series'].copy(), description['series_blocks']\n            \n            values = np.zeros((series_blocks[-1][2], 4))\n            for series_block in series_blocks:\n                i, begin, end = series_block\n                values[begin:end, 0:3] += predictions[i]\n                values[begin:end, 3] += 1\n\n            values = values[:len(series)]\n            \n            series['StartHesitation_prediction'] = values[:, 0] / values[:, 3]\n            series['Turn_prediction'] = values[:, 1] / values[:, 3]\n            series['Walking_prediction'] = values[:, 2] / values[:, 3]\n            series['Prediction_count'] = values[:, 3]\n            series['Event_prediction'] = series[['StartHesitation_prediction', 'Turn_prediction', 'Walking_prediction']].aggregate('max', axis=1)\n            \n            return series\n            \n        targets = Parallel(n_jobs=-1)(delayed(create_target)(self.descriptions[i]) for i in tqdm(range(len(self.descriptions)), disable=1-self.verbose))\n        targets = pd.concat(targets).reset_index(drop=True)\n        \n        return targets\n    \n    def on_epoch_end(self, epoch, logs=None):\n        scores=[]\n        scores.append(f'{(epoch+1):03d}')\n        \n        loss = logs['loss'] if epoch >= 0 else 1.0\n        \n        targets = self.prediction()\n        targets = targets[(targets['Task'] & targets['Valid'])]\n        \n        # Score\n        \n        defog_targets = targets[~targets['Turn'].isna()]\n            \n        Turn_mAP = average_precision_score(defog_targets['Turn'], defog_targets['Turn_prediction'])\n        Walking_mAP = average_precision_score(defog_targets['Walking'], defog_targets['Walking_prediction'])\n        mAP = (Walking_mAP+Turn_mAP)/2\n\n        print(f'\\n\\n[0] Turn mAP - {Turn_mAP:.3f} Walking mAP - {Walking_mAP:.3f} mAP - {mAP:.3f}')\n        \n        scores.append(f'{mAP:.3f}')\n        \n        # Score\n        \n        Event_mAP = average_precision_score(targets['Event'], targets['Event_prediction'])\n        \n        print(f'[1] Event mAP - {Event_mAP:.3f}\\n')\n        \n        scores.append(f'{Event_mAP:.3f}')\n        \n        # Save\n        \n        scores.append(f'{loss:.4f}')\n        \n        save_name = '_'.join(scores)\n        save_path = f'/kaggle/working/{save_name}_model.h5'\n        self.model.save_weights(save_path)\n        \n'''\nTraining\n        \n'''\n        \nLEARNING_RATE = 0.01/62\nSTEPS_PER_EPOCH = 256\nEPOCHS = 32\nWEIGHTS = ''\n\nif CFG['TPU']:\n    with tpu_strategy.scope():\n        model = FOGModel()\n        model.build(input_shape=(GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))\n        if len(WEIGHTS): model.load_weights(WEIGHTS)\n        model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, STEPS_PER_EPOCH), beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n        !rm -r /kaggle/working/*\n        model.fit(dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[PredictionFnCallback()])\nelse:\n    model = FOGModel()\n    model.build(input_shape=(GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))\n    if len(WEIGHTS): model.load_weights(WEIGHTS)\n    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, STEPS_PER_EPOCH), beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n    !rm -r /kaggle/working/*\n    model.fit(dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[PredictionFnCallback()])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T12:43:26.716528Z","iopub.execute_input":"2023-06-25T12:43:26.717082Z","iopub.status.idle":"2023-06-25T15:05:02.231909Z","shell.execute_reply.started":"2023-06-25T12:43:26.717049Z","shell.execute_reply":"2023-06-25T15:05:02.230857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nSearch for saved models in the working directory and sort them\n\n'''\n\nmodels = []\nfor fname in os.listdir('/kaggle/working/'):\n    if 'model.h5' in fname:\n        m = {}\n        m['Path'] = '/kaggle/working/' + fname\n        for i, elem in enumerate(fname.split('_')): \n            try:\n                m[i+1] = float(elem)\n            except:\n                m[i+1] = elem\n        models.append(m)\n\nif len(models): \n    models = pd.DataFrame(models)\n    plot(models.sort_values(1)[2].values)\n    models = models.sort_values(2, ascending=False)\n    display(models.head(15))","metadata":{"execution":{"iopub.status.busy":"2023-06-25T15:05:02.234103Z","iopub.execute_input":"2023-06-25T15:05:02.234502Z","iopub.status.idle":"2023-06-25T15:05:02.541092Z","shell.execute_reply.started":"2023-06-25T15:05:02.234445Z","shell.execute_reply":"2023-06-25T15:05:02.540146Z"},"trusted":true},"execution_count":null,"outputs":[]}]}